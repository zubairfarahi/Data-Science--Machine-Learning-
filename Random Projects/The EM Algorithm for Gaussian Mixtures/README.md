
# The Expectation Maximization (EM) algorithm 

The Expectation Maximization (EM) algorithm is a general iterative method to
find the maximum likelihood estimates of parameters in a statistical model. So
far, we’ve applied the EM algorithm in the context of Bayesian Networks. In
this context we were given some samples that contains Missing Completely at
Random / Missing at Random data, and our goal was to infer the parameters
of the Bayesian Network (tables of probabilities).
We’ve previously mentioned that the EM algorithm is a general method that
can be applied to a variety of other problems such as: Hidden Markov Models
and Gaussian Mixtures. In this homework we will apply the EM algorithm
to a Gaussian Mixture model, trying to infer the underling parameters of the
generative statistical model as we will further discuss



## Screenshots

![App Screenshot](https://github.com/zubairfarahi/Data-Science--Machine-Learning-/blob/main/Random%20Projects/The%20EM%20Algorithm%20for%20Gaussian%20Mixtures/1.PNG)

![App Screenshot](https://github.com/zubairfarahi/Data-Science--Machine-Learning-/blob/main/Random%20Projects/The%20EM%20Algorithm%20for%20Gaussian%20Mixtures/2.PNG)


